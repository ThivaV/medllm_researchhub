{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(message, history, api_key):\n",
    "    num_images = len(message[\"files\"])\n",
    "    total_images = 0\n",
    "    for message in history:\n",
    "        if isinstance(message[\"content\"], tuple):\n",
    "            total_images += 1\n",
    "    return f\"You just uploaded {num_images} images, total uploaded: {total_images+num_images}, api_key: {api_key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def respond(message, chat_history, api_key):\n",
    "#     if not api_key.strip():\n",
    "#         return chat_history + [(message, \"⚠️ Please enter your GROQ API key first!\")]\n",
    "    \n",
    "#     try:\n",
    "#         client = Groq(api_key=api_key.strip())\n",
    "        \n",
    "#         # Build conversation history\n",
    "#         messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "#         for user_msg, bot_msg in chat_history:\n",
    "#             messages.extend([\n",
    "#                 {\"role\": \"user\", \"content\": user_msg},\n",
    "#                 {\"role\": \"assistant\", \"content\": bot_msg}\n",
    "#             ])\n",
    "#         messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "#         # Get response\n",
    "#         response = client.chat.completions.create(\n",
    "#             messages=messages,\n",
    "#             model=\"llama3-70b-8192\",\n",
    "#             temperature=0.7,\n",
    "#             max_tokens=1024\n",
    "#         )\n",
    "        \n",
    "#         bot_message = response.choices[0].message.content\n",
    "#         return chat_history + [(message, bot_message)]\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         return chat_history + [(message, f\"❌ Error: {str(e)}\")]\n",
    "    \n",
    "# def chat(message, history):\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": message}\n",
    "#     ]\n",
    "    \n",
    "#     response = client.chat.completions.create(\n",
    "#         messages=messages,\n",
    "#         model=model,\n",
    "#         temperature=0.7,  # Control randomness (0=deterministic, 1=creative)\n",
    "#         max_tokens=1024    # Limit response length\n",
    "#     )\n",
    "#     return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with gr.Blocks() as demo:\n",
    "#     gr.Markdown(\"## Groq Chatbot - Enter API Key to Start\")\n",
    "    \n",
    "#     # API Key input (always visible)\n",
    "#     api_key = gr.Textbox(\n",
    "#         label=\"GROQ API Key\",\n",
    "#         placeholder=\"Enter your key here...\",\n",
    "#         type=\"password\"\n",
    "#     )\n",
    "    \n",
    "#     # # Chat interface\n",
    "#     # chatbot = gr.Chatbot(height=400)\n",
    "#     # msg = gr.Textbox(label=\"Your Message\")\n",
    "#     # clear_btn = gr.ClearButton([msg, chatbot, api_key])\n",
    "    \n",
    "#     # # Chat handling\n",
    "#     # msg.submit(\n",
    "#     #     respond,\n",
    "#     #     [msg, chatbot, api_key],\n",
    "#     #     [chatbot],\n",
    "#     #     queue=False\n",
    "#     # )\n",
    "\n",
    "#     # Chat interface\n",
    "#     chat = gr.ChatInterface(\n",
    "#         title=\"BioMED\",\n",
    "#         description=\"## A description for the interface; if provided, appears above the chatbot and beneath the title in regular font. Accepts Markdown and HTML content.\",\n",
    "#         theme=\"gradio/monochrome\",\n",
    "#         show_progress=\"full\",\n",
    "#         fill_height=True,\n",
    "#         fill_width=True,\n",
    "#         # save_history=True,\n",
    "#         fn=count_images,\n",
    "#         # additional_inputs=api_key, \n",
    "#         type=\"messages\", \n",
    "#         # examples=[\n",
    "#         #     {\"text\": \"No files\", \"files\": []}\n",
    "#         # ], \n",
    "#         multimodal=True,\n",
    "#     )\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7896\n",
      "* Running on public URL: https://b52d3068e1180aae98.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b52d3068e1180aae98.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpful assistant.'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/mnt/d/vault/devhub/medllm_researchhub/.venv/lib/python3.13/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/mnt/d/vault/devhub/medllm_researchhub/.venv/lib/python3.13/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/mnt/d/vault/devhub/medllm_researchhub/.venv/lib/python3.13/site-packages/gradio/blocks.py\", line 2044, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/mnt/d/vault/devhub/medllm_researchhub/.venv/lib/python3.13/site-packages/gradio/blocks.py\", line 1589, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/d/vault/devhub/medllm_researchhub/.venv/lib/python3.13/site-packages/gradio/utils.py\", line 850, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/d/vault/devhub/medllm_researchhub/.venv/lib/python3.13/site-packages/gradio/chat_interface.py\", line 869, in _submit_fn\n",
      "    history = self._append_message_to_history(response, history, \"assistant\")\n",
      "  File \"/mnt/d/vault/devhub/medllm_researchhub/.venv/lib/python3.13/site-packages/gradio/chat_interface.py\", line 800, in _append_message_to_history\n",
      "    message_dicts = self._message_as_message_dict(message, role)\n",
      "  File \"/mnt/d/vault/devhub/medllm_researchhub/.venv/lib/python3.13/site-packages/gradio/chat_interface.py\", line 838, in _message_as_message_dict\n",
      "    for x in msg.get(\"files\", []):\n",
      "             ^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'get'\n"
     ]
    }
   ],
   "source": [
    "def respond(message, chat_history, api_key):\n",
    "    if not api_key.strip():\n",
    "        return chat_history + [(message, \"⚠️ Please enter your GROQ API key first!\")]\n",
    "    \n",
    "    try:\n",
    "        client = Groq(api_key=api_key.strip())\n",
    "        \n",
    "        # Build conversation history\n",
    "        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "        for user_msg, bot_msg in chat_history:\n",
    "            messages.extend([\n",
    "                {\"role\": \"user\", \"content\": user_msg},\n",
    "                {\"role\": \"assistant\", \"content\": bot_msg}\n",
    "            ])\n",
    "        print(messages)    \n",
    "        messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        # Get response\n",
    "        response = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=\"llama-3.2-90b-vision-preview\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        bot_message = response.choices[0].message.content\n",
    "        return chat_history + [(message, bot_message)]\n",
    "    \n",
    "    except Exception as e:\n",
    "        return chat_history + [(message, f\"❌ Error: {str(e)}\")]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Accordion(\"Enter your Groq key!\", open=False):\n",
    "        api_key = gr.Textbox(\n",
    "            label=\"GROQ API Key\",\n",
    "            placeholder=\"Enter your key here...\",\n",
    "            type=\"password\"\n",
    "        )\n",
    "\n",
    "    with gr.Accordion(\"BioMEDICAL VisionLM AI Tool\", open=True):\n",
    "        gr.ChatInterface(\n",
    "            title=\"BioMED⚕️\",\n",
    "            description=\"> 🚨 This application is designed as a comprehensive AI tool for medical analysis, leveraging advanced multimodal capabilities to assist healthcare professionals and potentially extend access to underserved communities.\",\n",
    "            theme=\"gradio/monochrome\",\n",
    "            show_progress=\"full\",\n",
    "            fill_height=True,\n",
    "            fill_width=True,\n",
    "            # save_history=True,\n",
    "            fn=respond,\n",
    "            additional_inputs=api_key, \n",
    "            type=\"messages\", \n",
    "            # examples=[\n",
    "            #     {\"text\": \"No files\", \"files\": []}\n",
    "            # ], \n",
    "            multimodal=True,\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
